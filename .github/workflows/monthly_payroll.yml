name: Hyper-Scale Financial Batch System

on:
  schedule:
    - cron: '0 9 11,21 * *'
  workflow_dispatch:

jobs:
  run-hyperscale-system:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: read
      checks: write

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install Dependencies
      run: |
        pip install pandas matplotlib reportlab openpyxl numpy cryptography tabulate schedule psutil

    - name: Generate Hyper-Scale Script
      run: |
        mkdir -p scripts
        cat << 'EOF' > scripts/hyperscale_system.py
        import sqlite3
        import pandas as pd
        import os
        import time
        import shutil
        import schedule
        import numpy as np
        import hashlib
        import psutil # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
        from datetime import datetime
        from cryptography.fernet import Fernet
        from tabulate import tabulate

        # ==========================================
        # 1. Configuration (Hyper-Scale)
        # ==========================================
        BASE_DIR = os.getcwd()
        DIRS = {
            'INBOUND': os.path.join(BASE_DIR, 'INBOUND'),
            'ARCHIVE': os.path.join(BASE_DIR, 'DATA/ARCHIVE'),
            'BACKUP': os.path.join(BASE_DIR, 'DATA/BACKUP'),
            'REPORT': os.path.join(BASE_DIR, 'DATA/REPORT'),
            'TEMP': os.path.join(BASE_DIR, 'DATA/TEMP')
        }
        DOMAINS = ['CORE', 'LOAN', 'CARD', 'STOCK', 'FOREX']
        
        # [íŠœë‹] ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
        CHUNK_SIZE = 10000   # í•œ ë²ˆì— ì²˜ë¦¬í•  í–‰ ìˆ˜ (ë©”ëª¨ë¦¬ ìµœì í™”)
        GEN_COUNT = 50000    # í•œ ë²ˆì— ìƒì„±í•  íŠ¸ëœì­ì…˜ ìˆ˜ (ë¶€í•˜ í…ŒìŠ¤íŠ¸ìš©)
        
        for d in DIRS.values():
            if not os.path.exists(d): os.makedirs(d)
        for dom in DOMAINS:
            p = os.path.join(DIRS['INBOUND'], dom)
            if not os.path.exists(p): os.makedirs(p)

        # ==========================================
        # 2. High-Performance DB Manager
        # ==========================================
        class HyperScaleDW:
            def __init__(self, db_name="hyperscale_dw.db"):
                self.db_path = os.path.join(BASE_DIR, 'DATA', db_name)
                self.conn = sqlite3.connect(self.db_path)
                self.cursor = self.conn.cursor()
                
                # [í•µì‹¬] ê³ ì„±ëŠ¥ íŠœë‹ (WAL ëª¨ë“œ & ë™ê¸°í™” í•´ì œ)
                self.conn.execute("PRAGMA journal_mode = WAL;") 
                self.conn.execute("PRAGMA synchronous = OFF;")
                self.conn.execute("PRAGMA cache_size = 10000;") # ë©”ëª¨ë¦¬ ìºì‹œ ì¦ì„¤
                
                self._init_schema()

            def _init_schema(self):
                # íŒŒí‹°ì…”ë‹ ê°œë…ì„ ë„ì…í•˜ê¸°ì—” SQLiteê°€ ì‘ì§€ë§Œ, ì¸ë±ì‹±ìœ¼ë¡œ ìµœì í™”
                self.cursor.execute('''CREATE TABLE IF NOT EXISTS fact_large_ledger (
                    tx_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    domain TEXT,
                    tx_type TEXT,
                    user_id TEXT, -- ì•”í˜¸í™”ëœ ID
                    amount INTEGER,
                    tx_time TIMESTAMP,
                    batch_id TEXT
                )''')
                
                # ì¸ë±ìŠ¤ ìƒì„± (ì¡°íšŒ ì„±ëŠ¥ í–¥ìƒ)
                self.cursor.execute("CREATE INDEX IF NOT EXISTS idx_domain ON fact_large_ledger(domain)")
                self.cursor.execute("CREATE INDEX IF NOT EXISTS idx_time ON fact_large_ledger(tx_time)")
                
                self.cursor.execute('''CREATE TABLE IF NOT EXISTS sys_job_log (
                    job_id TEXT, start_time TIMESTAMP, end_time TIMESTAMP, 
                    status TEXT, processed_rows INTEGER, message TEXT
                )''')
                self.conn.commit()

            def bulk_insert(self, data_tuples):
                """ executemanyë¥¼ ì‚¬ìš©í•œ ì´ˆê³ ì† ì ì¬ """
                try:
                    self.cursor.executemany("""
                        INSERT INTO fact_large_ledger (domain, tx_type, user_id, amount, tx_time, batch_id)
                        VALUES (?, ?, ?, ?, datetime('now'), ?)
                    """, data_tuples)
                    self.conn.commit()
                    return True
                except Exception as e:
                    print(f"DB Error: {e}")
                    return False

            def execute_backup(self):
                ts = datetime.now().strftime('%Y%m%d_%H%M%S')
                bk_path = os.path.join(DIRS['BACKUP'], f"BACKUP_{ts}.db")
                try:
                    bck = sqlite3.connect(bk_path)
                    self.conn.backup(bck)
                    bck.close()
                    return True, bk_path
                except Exception as e:
                    return False, str(e)

        # ==========================================
        # 3. Security (AES-256) - Performance Optimized
        # ==========================================
        class SecurityManager:
            def __init__(self):
                self.key = Fernet.generate_key()
                self.cipher = Fernet(self.key)
            
            def encrypt_batch(self, text_list):
                """ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ë°ì´í„°ë¥¼ ì¼ê´„ ì•”í˜¸í™” (Function Call ì˜¤ë²„í—¤ë“œ ê°ì†Œ)"""
                return [self.cipher.encrypt(str(t).encode()).decode() for t in text_list]

        # ==========================================
        # 4. Big Data Generator (Numpy based)
        # ==========================================
        class BigDataGenerator:
            def gen_massive_file(self, domain):
                ts = datetime.now().strftime("%H%M%S")
                fname = f"{domain}_BIGDATA_{ts}.csv"
                fpath = os.path.join(DIRS['INBOUND'], domain, fname)
                
                # Numpyë¡œ ëŒ€ëŸ‰ ë°ì´í„° ê³ ì† ìƒì„±
                ids = np.random.randint(1000000, 9999999, size=GEN_COUNT)
                amounts = np.random.randint(1000, 10000000, size=GEN_COUNT)
                types = np.random.choice(['PAYMENT', 'TRANSFER', 'DEPOSIT', 'WITHDRAW'], size=GEN_COUNT)
                
                df = pd.DataFrame({
                    'user_id': ids,
                    'type': types,
                    'amount': amounts
                })
                
                df.to_csv(fpath, index=False)
                size_mb = os.path.getsize(fpath) / (1024 * 1024)
                print(f"ğŸš€ [GEN] {domain} ëŒ€ìš©ëŸ‰ íŒŒì¼ ìƒì„±: {fname} ({GEN_COUNT} rows, {size_mb:.2f} MB)")

        # ==========================================
        # 5. Control-M Engine (Chunk Processing)
        # ==========================================
        class ControlM_Engine:
            def __init__(self, dw):
                self.dw = dw
                self.sec = SecurityManager()

            def job_ctmfw(self):
                """ëª¨ë“  ë„ë©”ì¸ í´ë” ê°ì‹œ"""
                for dom in DOMAINS:
                    watch_dir = os.path.join(DIRS['INBOUND'], dom)
                    files = [f for f in os.listdir(watch_dir) if f.endswith('.csv')]
                    
                    for f in files:
                        src = os.path.join(watch_dir, f)
                        job_id = f"JOB_LOAD_{dom}_{int(time.time())}"
                        
                        print(f"ğŸ‘ï¸ [CTMFW] íŒŒì¼ ê°ì§€: {f} -> ë°°ì¹˜ ë¡œë” ê°€ë™")
                        self.run_chunk_loader(dom, src, job_id)
                        
                        # ì²˜ë¦¬ ì™„ë£Œ í›„ ì´ë™
                        dst = os.path.join(DIRS['ARCHIVE'], f"{dom}_{f}")
                        shutil.move(src, dst)

            def run_chunk_loader(self, domain, filepath, job_id):
                """Chunk ë‹¨ìœ„ë¡œ íŒŒì¼ì„ ì½ì–´ ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜ ì—†ì´ ì ì¬"""
                start_time = datetime.now()
                total_rows = 0
                
                try:
                    # [í•µì‹¬] Pandas Chunksize ì˜µì…˜ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
                    for chunk in pd.read_csv(filepath, chunksize=CHUNK_SIZE):
                        # ë³´ì•ˆ: ì‚¬ìš©ì ID ì¼ê´„ ì•”í˜¸í™”
                        enc_ids = self.sec.encrypt_batch(chunk['user_id'].tolist())
                        
                        # íŠœí”Œ ë³€í™˜ (ì†ë„ ìµœì í™”)
                        data_tuples = list(zip(
                            [domain] * len(chunk),
                            chunk['type'],
                            enc_ids,
                            chunk['amount'],
                            [job_id] * len(chunk)
                        ))
                        
                        # DB ì ì¬
                        self.dw.bulk_insert(data_tuples)
                        total_rows += len(chunk)
                        print(f"   â†³ [Loader] {len(chunk)} rows processed... (Total: {total_rows})")

                    self.dw.cursor.execute("INSERT INTO sys_job_log VALUES (?,?,?,?,?,?)",
                        (job_id, start_time, datetime.now(), "SUCCESS", total_rows, "Bulk Load OK"))
                    self.dw.conn.commit()
                    
                    # ë°±ì—… íŠ¸ë¦¬ê±°
                    self.dw.execute_backup()
                    
                except Exception as e:
                    print(f"   !! Error: {e}")
                    self.dw.cursor.execute("INSERT INTO sys_job_log VALUES (?,?,?,?,?,?)",
                        (job_id, start_time, datetime.now(), "FAIL", total_rows, str(e)))
                    self.dw.conn.commit()

        # ==========================================
        # 6. Monitoring & Main
        # ==========================================
        def show_status(dw):
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
            mem = psutil.virtual_memory()
            print(f"\n[Monitor] CPU: {psutil.cpu_percent()}% | RAM: {mem.percent}% Used")
            
            # DB í†µê³„
            cnt = dw.cursor.execute("SELECT count(*) FROM fact_large_ledger").fetchone()[0]
            print(f"[DB Stat] Total Transactions: {cnt:,} rows")
            
            # Job ë¡œê·¸
            df = pd.read_sql("SELECT job_id, status, processed_rows, end_time FROM sys_job_log ORDER BY end_time DESC LIMIT 3", dw.conn)
            if not df.empty:
                print(tabulate(df, headers='keys', tablefmt='plain'))
            print("-" * 50)

        if __name__ == "__main__":
            print(">> [Init] Hyper-Scale Financial System Started")
            dw = HyperScaleDW()
            ctm = ControlM_Engine(dw)
            gen = BigDataGenerator()
            
            # ìŠ¤ì¼€ì¤„ë§
            # 1. ëŒ€ìš©ëŸ‰ íŒŒì¼ ìƒì„± (ê°ê¸° ë‹¤ë¥¸ ì£¼ê¸°)
            schedule.every(5).seconds.do(lambda: gen.gen_massive_file('CORE'))
            schedule.every(8).seconds.do(lambda: gen.gen_massive_file('CARD'))
            
            # 2. CTMFW (ë¹ ë¥¸ ê°ì‹œ)
            schedule.every(1).seconds.do(ctm.job_ctmfw)
            
            # 3. ëª¨ë‹ˆí„°ë§
            schedule.every(3).seconds.do(lambda: show_status(dw))
            
            # ì‹¤í–‰ ë£¨í”„ (90ì´ˆê°„ ìˆ˜í–‰í•˜ì—¬ ë°ì´í„° ì¶•ì  í™•ì¸)
            start_ts = time.time()
            while time.time() - start_ts < 90:
                schedule.run_pending()
                time.sleep(1)
            
            # ìµœì¢… ë¦¬í¬íŠ¸ (ë„ˆë¬´ í¬ë¯€ë¡œ ìš”ì•½ë§Œ ì €ì¥)
            print(">> [Shutdown] Exporting Summary Reports...")
            query = "SELECT domain, tx_type, count(*) as cnt, sum(amount) as total FROM fact_large_ledger GROUP BY domain, tx_type"
            pd.read_sql(query, dw.conn).to_excel(f"{DIRS['REPORT']}/Summary_Report.xlsx")
        EOF

    - name: Run Hyper-Scale System
      run: python scripts/hyperscale_system.py

    - name: Upload Artifacts
      uses: actions/upload-artifact@v4
      with:
        name: HyperScale-Data-${{ github.run_id }}
        path: |
          DATA/REPORT/
          DATA/BACKUP/
        retention-days: 30
