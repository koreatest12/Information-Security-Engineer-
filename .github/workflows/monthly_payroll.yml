name: Ultimate Korea Financial Platform

on:
  schedule:
    - cron: '0 17 * * *' # í•œêµ­ ì‹œê°„ 02:00 (UTC 17:00) í†µí•© ë°°ì¹˜
  workflow_dispatch:

jobs:
  run-unified-financial-system:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: read
      checks: write

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install Enterprise Dependencies
      run: |
        pip install pandas numpy cryptography schedule tabulate psutil openpyxl networkx

    - name: Deploy Unified System Engine
      run: |
        mkdir -p scripts
        cat << 'EOF' > scripts/unified_fin_system.py
        import os
        import time
        import sqlite3
        import shutil
        import schedule
        import pandas as pd
        import numpy as np
        import networkx as nx
        import concurrent.futures
        from datetime import datetime
        from cryptography.fernet import Fernet
        from tabulate import tabulate

        # ==========================================
        # 1. Global Configuration (Infrastructure)
        # ==========================================
        BASE_DIR = os.getcwd()
        
        # [ë””ë ‰í† ë¦¬ í†µí•© ì •ì˜]
        DIRS = {
            # ì‹œìŠ¤í…œ ë¡œê·¸ ë° ì„¤ì •
            'LOGS_INSTALL': os.path.join(BASE_DIR, 'LOGS/INSTALL'),
            'LOGS_CTM': os.path.join(BASE_DIR, 'LOGS/CTM'),
            
            # DB í´ëŸ¬ìŠ¤í„° ë° ë°ì´í„°
            'DB_CLUSTER': os.path.join(BASE_DIR, 'INFRA/DB_CLUSTER'),
            'DATA_CORE': os.path.join(BASE_DIR, 'DATA/CORE_BANK'),
            'DATA_CARD': os.path.join(BASE_DIR, 'DATA/CARD_NET'),
            'DATA_STOCK': os.path.join(BASE_DIR, 'DATA/CAPITAL'),
            
            # ETL ë° DW
            'ETL_SOURCE': os.path.join(BASE_DIR, 'DATA/SOURCE'),
            'ETL_WAREHOUSE': os.path.join(BASE_DIR, 'DATA/WAREHOUSE'),
            
            # ë°±ì—… ì„¼í„°
            'BACKUP_SQL': os.path.join(BASE_DIR, 'BACKUP/SQL_DUMPS'),
            'BACKUP_SNAP': os.path.join(BASE_DIR, 'BACKUP/SNAPSHOTS'),
            'BACKUP_TLOG': os.path.join(BASE_DIR, 'BACKUP/T_LOGS'),
            
            # ë¦¬í¬íŠ¸
            'REPORTS': os.path.join(BASE_DIR, 'REPORTS')
        }

        # ì‹œë®¬ë ˆì´ì…˜ íƒ€ê²Ÿ ë²¤ë”
        TARGET_VENDORS = ['ORACLE_19c', 'MSSQL_2022', 'POSTGRES_15', 'MYSQL_8', 'MONGODB_6']

        # ì´ˆê¸°í™”
        for d in DIRS.values():
            os.makedirs(d, exist_ok=True)

        # ==========================================
        # 2. Control-M Orchestration Engine
        # ==========================================
        class ControlM_Enterprise:
            def __init__(self):
                self.job_status = {}
                self.dag = nx.DiGraph()
                
            def define_job(self, job_name, func, depends_on=None):
                self.job_status[job_name] = "WAITING"
                self.dag.add_node(job_name, cmd=func)
                if depends_on:
                    for parent in depends_on:
                        self.dag.add_edge(parent, job_name)
            
            def run_flow(self):
                print(f"\nðŸŽ® [Control-M] Starting Unified Batch Flow...")
                try:
                    order = list(nx.topological_sort(self.dag))
                except:
                    print("ðŸš¨ [Control-M] Cycle Detected! Aborting.")
                    return

                for job in order:
                    # ì¢…ì†ì„± ì²´í¬
                    parents = list(self.dag.predecessors(job))
                    if not all(self.job_status[p] == "ENDED_OK" for p in parents):
                        print(f"   â›” [Skip] {job} (Dependency Failed)")
                        self.job_status[job] = "ENDED_NOTOK"
                        continue

                    # ì‹¤í–‰
                    print(f"   â–¶ï¸ [Start] {job} ...")
                    self.job_status[job] = "RUNNING"
                    start = time.time()
                    try:
                        res = self.dag.nodes[job]['cmd']()
                        sec = time.time() - start
                        self.job_status[job] = "ENDED_OK"
                        print(f"      âœ… [OK] {job} ({sec:.2f}s) - {res}")
                        
                        # ê°ì‚¬ ë¡œê·¸
                        with open(f"{DIRS['LOGS_CTM']}/job_history.log", "a") as f:
                            f.write(f"{datetime.now()}|{job}|ENDED_OK|{res}\n")
                    except Exception as e:
                        self.job_status[job] = "ENDED_NOTOK"
                        print(f"      âŒ [FAIL] {job} - {e}")

        # ==========================================
        # 3. DB Cluster Manager (Mass Installer)
        # ==========================================
        class DBClusterManager:
            def __init__(self):
                self.installed_dbs = []

            def _install_one(self, vendor):
                p = os.path.join(DIRS['DB_CLUSTER'], vendor)
                os.makedirs(p, exist_ok=True)
                # Config Mock
                with open(os.path.join(p, 'init.ora'), 'w') as f:
                    f.write(f"DB_NAME={vendor}\nMEMORY=16G")
                return f"{vendor} Ready"

            def job_mass_install(self):
                """[Infrastructure] 5ëŒ€ DB ë³‘ë ¬ ì„¤ì¹˜"""
                with concurrent.futures.ThreadPoolExecutor(max_workers=5) as exc:
                    futures = [exc.submit(self._install_one, v) for v in TARGET_VENDORS]
                    for f in concurrent.futures.as_completed(futures):
                        pass 
                self.installed_dbs = TARGET_VENDORS
                return f"Deployed {len(TARGET_VENDORS)} DB Clusters"

        # ==========================================
        # 4. Korea Financial Business Logic
        # ==========================================
        class KoreaFinancialSystem:
            def __init__(self):
                self.key = Fernet.generate_key()
                self.cipher = Fernet(self.key)
                self.ledger_path = os.path.join(BASE_DIR, 'korea_main_ledger.db')
                self._init_db()

            def _init_db(self):
                conn = sqlite3.connect(self.ledger_path)
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute('''CREATE TABLE IF NOT EXISTS tx_ledger (
                    tx_id TEXT, domain TEXT, code TEXT, 
                    enc_acc TEXT, amount INT, timestamp DATETIME)''')
                conn.close()

            def _save(self, df):
                conn = sqlite3.connect(self.ledger_path)
                df.to_sql('tx_ledger', conn, if_exists='append', index=False)
                conn.close()

            def job_core_banking(self):
                """[Biz] ìˆ˜ì‹ /ì—¬ì‹ /ì´ì²´ ì²˜ë¦¬"""
                cnt = 3000
                df = pd.DataFrame({
                    'tx_id': [f"TX_CORE_{i}" for i in range(cnt)],
                    'domain': 'CORE',
                    'code': np.random.choice(['DEPOSIT', 'WITHDRAW', 'TRANSFER'], cnt),
                    'enc_acc': [self.cipher.encrypt(f"ACC_{i}".encode()).decode() for i in range(cnt)],
                    'amount': np.random.randint(1000, 1000000, cnt),
                    'timestamp': datetime.now()
                })
                self._save(df)
                return f"Processed {cnt} Core Tx"

            def job_card_approval(self):
                """[Biz] ì¹´ë“œ ìŠ¹ì¸ ë° ë§¤ìž…"""
                cnt = 5000
                df = pd.DataFrame({
                    'tx_id': [f"TX_CARD_{i}" for i in range(cnt)],
                    'domain': 'CARD',
                    'code': 'APPROVAL',
                    'enc_acc': [self.cipher.encrypt(f"CARD_{i}".encode()).decode() for i in range(cnt)],
                    'amount': np.random.randint(500, 50000, cnt),
                    'timestamp': datetime.now()
                })
                self._save(df)
                return f"Settled {cnt} Card Tx"

            def job_stock_trading(self):
                """[Biz] ì£¼ì‹ ì²´ê²°"""
                cnt = 2000
                df = pd.DataFrame({
                    'tx_id': [f"TX_STK_{i}" for i in range(cnt)],
                    'domain': 'STOCK',
                    'code': np.random.choice(['BUY', 'SELL'], cnt),
                    'enc_acc': [self.cipher.encrypt(f"STK_{i}".encode()).decode() for i in range(cnt)],
                    'amount': np.random.randint(10000, 2000000, cnt),
                    'timestamp': datetime.now()
                })
                self._save(df)
                return f"Executed {cnt} Trades"

        # ==========================================
        # 5. Secure ETL & Warehouse
        # ==========================================
        class HyperScaleETL:
            def __init__(self, fin_sys):
                self.fin = fin_sys
                self.dw_path = os.path.join(DIRS['ETL_WAREHOUSE'], 'master_dw.db')
                
            def job_etl_pipeline(self):
                """[ETL] ì›ìž¥ ë°ì´í„° ì¶”ì¶œ -> ë³€í™˜ -> DW ì ìž¬"""
                src_conn = sqlite3.connect(self.fin.ledger_path)
                df = pd.read_sql("SELECT * FROM tx_ledger", src_conn)
                src_conn.close()
                
                if df.empty: return "No Data to Load"
                
                # Transform (ì§‘ê³„)
                agg = df.groupby(['domain', 'code'])['amount'].sum().reset_index()
                agg['batch_time'] = datetime.now()
                
                # Load
                tgt_conn = sqlite3.connect(self.dw_path)
                agg.to_sql('daily_summary', tgt_conn, if_exists='append', index=False)
                tgt_conn.close()
                
                return f"Loaded {len(df)} rows into DW Summary"

        # ==========================================
        # 6. Universal Omni-Backup
        # ==========================================
        class UniversalBackupAgent:
            def __init__(self, db_mgr, fin_sys):
                self.db_mgr = db_mgr
                self.fin_sys = fin_sys
                
            def job_backup_all(self):
                """[Backup] ì¸í”„ë¼ ë° ë°ì´í„° ì „ì²´ ë°±ì—…"""
                ts = datetime.now().strftime("%H%M%S")
                results = []
                
                # 1. Vendor DB ë°±ì—… (Cluster)
                for vendor in self.db_mgr.installed_dbs:
                    if 'ORACLE' in vendor:
                        fname = f"RMAN_{vendor}_{ts}.bkp"
                        path = os.path.join(DIRS['BACKUP_SNAP'], fname)
                        with open(path, 'w') as f: f.write("ORACLE RMAN DATA")
                        results.append(f"Oracle RMAN: {fname}")
                    elif 'MSSQL' in vendor:
                        fname = f"TLOG_{vendor}_{ts}.trn"
                        path = os.path.join(DIRS['BACKUP_TLOG'], fname)
                        with open(path, 'w') as f: f.write("MSSQL T-LOG")
                        results.append(f"MSSQL T-Log: {fname}")
                
                # 2. ê¸ˆìœµ ì›ìž¥ DB ë°±ì—…
                ledger_bk = os.path.join(DIRS['BACKUP_SNAP'], f"LEDGER_FULL_{ts}.db")
                shutil.copy2(self.fin_sys.ledger_path, ledger_bk)
                results.append("Ledger DB Backed up")
                
                return " / ".join(results)

        # ==========================================
        # 7. Main Execution (Workflow Definition)
        # ==========================================
        if __name__ == "__main__":
            print(">> [System] Booting Ultimate Korea Financial Platform...")
            
            # ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
            ctm = ControlM_Enterprise()
            db_cluster = DBClusterManager()
            fin_sys = KoreaFinancialSystem()
            etl = HyperScaleETL(fin_sys)
            backup = UniversalBackupAgent(db_cluster, fin_sys)
            
            # --- [Control-M Job Definition] ---
            
            # 1. ì¸í”„ë¼ ì¤€ë¹„ (ê°€ìž¥ ë¨¼ì € ìˆ˜í–‰)
            ctm.define_job("INFRA_DB_INSTALL", db_cluster.job_mass_install)
            
            # 2. ê¸ˆìœµ ê±°ëž˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (ì¸í”„ë¼ ì™„ë£Œ í›„ ë³‘ë ¬ ì‹¤í–‰)
            ctm.define_job("BIZ_CORE_BANK", fin_sys.job_core_banking, depends_on=["INFRA_DB_INSTALL"])
            ctm.define_job("BIZ_CARD_APPR", fin_sys.job_card_approval, depends_on=["INFRA_DB_INSTALL"])
            ctm.define_job("BIZ_STOCK_TRD", fin_sys.job_stock_trading, depends_on=["INFRA_DB_INSTALL"])
            
            # 3. ETL (ëª¨ë“  ê±°ëž˜ ì¢…ë£Œ í›„ ì‹¤í–‰)
            ctm.define_job("DATA_ETL_LOAD", etl.job_etl_pipeline, 
                           depends_on=["BIZ_CORE_BANK", "BIZ_CARD_APPR", "BIZ_STOCK_TRD"])
            
            # 4. ë°±ì—… (ETL ì™„ë£Œ í›„ ì‹¤í–‰)
            ctm.define_job("SYS_FULL_BACKUP", backup.job_backup_all, depends_on=["DATA_ETL_LOAD"])
            
            # 5. ë¦¬í¬íŒ… (ìµœì¢…)
            def job_final_report():
                conn = sqlite3.connect(etl.dw_path)
                df = pd.read_sql("SELECT * FROM daily_summary", conn)
                path = f"{DIRS['REPORTS']}/Final_EOD_Report.xlsx"
                df.to_excel(path)
                conn.close()
                return "Report Generated"
                
            ctm.define_job("RPT_EOD_GEN", job_final_report, depends_on=["SYS_FULL_BACKUP"])
            
            # --- [Execution Trigger] ---
            # ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„  ìŠ¤ì¼€ì¤„ëŸ¬ê°€ íŠ¸ë¦¬ê±°í•˜ì§€ë§Œ, ì—¬ê¸°ì„  ì¦‰ì‹œ ì‹¤í–‰
            ctm.run_flow()
            
            print("\n>> [System] Unified Workflow Completed Successfully.")
        EOF

    - name: Run Unified Platform
      run: python scripts/unified_fin_system.py

    - name: Upload Enterprise Artifacts
      uses: actions/upload-artifact@v4
      with:
        name: K-FinHub-Enterprise-Data-${{ github.run_id }}
        path: |
          REPORTS/
          LOGS/
          BACKUP/
        retention-days: 90
