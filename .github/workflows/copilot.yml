name: ğŸ‘ï¸ Grand Ops AI-Dashboard v20 (UI Integrated)

on:
  schedule:
    - cron: '*/10 * * * *' # 10ë¶„ ì£¼ê¸°
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: false

env:
  TZ: 'Asia/Seoul'
  DB_PATH: 'data/grand_ops_server.db'
  # ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ
  DAILY_REPORT: 'data/daily_insight.md'
  WEEKLY_REPORT: 'data/weekly_analysis.md'
  AUDIT_REPORT: 'data/security_audit.md'
  BACKUP_DIR: 'backup'
  ARTIFACT_PASSWORD: ${{ secrets.ARTIFACT_KEY || 'Dashboard_Secure_2025' }}

permissions:
  contents: write

jobs:
  ai-visual-dashboard:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 1. í™˜ê²½ ì„¤ì •
      - name: ğŸ—ï¸ Setup Visual Environment
        run: |
          echo "ğŸ§¬ Initializing Dashboard Stack..."
          mkdir -p data backup scripts
          python -m pip install --upgrade pip
          echo -e "pandas\nnumpy\nscikit-learn\nscipy\ntabulate" > requirements.txt
          pip install -r requirements.txt

      # 2. [Core] ë¦¬í¬íŒ… ì—”ì§„ ìŠ¤í¬ë¦½íŠ¸ (v20 - UI ìµœì í™”)
      - name: ğŸ§  Generate Dashboard Script
        run: |
          cat <<'EOF' > scripts/ai_dashboard_core.py
          import sqlite3
          import os
          import datetime
          import hashlib
          import sys
          import pandas as pd
          import numpy as np
          from sklearn.ensemble import IsolationForest
          from sklearn.linear_model import LinearRegression

          # ê²½ë¡œ ì„¤ì •
          DB_PATH = os.getenv('DB_PATH', 'data/grand_ops_server.db')
          DAILY_PATH = os.getenv('DAILY_REPORT', 'data/daily_insight.md')
          WEEKLY_PATH = os.getenv('WEEKLY_REPORT', 'data/weekly_analysis.md')
          AUDIT_PATH = os.getenv('AUDIT_REPORT', 'data/security_audit.md')

          class AuditSystem:
              def __init__(self, conn):
                  self.conn = conn
              
              def log(self, actor, action, details):
                  try:
                      raw = f"{actor}{action}{details}{datetime.datetime.now()}"
                      h_val = hashlib.sha256(raw.encode()).hexdigest()
                      self.conn.execute(
                          'INSERT INTO audit_trail (actor, action, details, data_hash) VALUES (?,?,?,?)', 
                          (actor, action, details, h_val)
                      )
                      self.conn.commit()
                  except Exception as e:
                      print(f"âš ï¸ Audit Error: {e}")

          class DataManager:
              def __init__(self, conn):
                  self.conn = conn

              def seed_data(self):
                  cursor = self.conn.cursor()
                  cursor.execute("SELECT count(*) FROM historical_data")
                  if cursor.fetchone()[0] < 10:
                      print("ğŸŒ± Seeding initial data...")
                      data = []
                      for i in range(50):
                          data.append((
                              np.random.normal(45, 12), 
                              np.random.randint(600, 1400), 
                              np.random.normal(22, 6),
                              (datetime.datetime.now() - datetime.timedelta(minutes=10*i))
                          ))
                      cursor.executemany('INSERT INTO historical_data (cpu_load, traffic, latency, timestamp) VALUES (?,?,?,?)', data)
                      self.conn.commit()

              def collect(self):
                  load = max(0, np.random.normal(50, 15))
                  traffic = max(0, int(np.random.poisson(1000)))
                  latency = max(0, np.random.exponential(20))
                  self.conn.execute('INSERT INTO historical_data (cpu_load, traffic, latency) VALUES (?,?,?)', (load, traffic, latency))
                  self.conn.commit()
                  return load, traffic

          class AIAnalyzer:
              def __init__(self, conn):
                  self.conn = conn
              
              def analyze(self):
                  df = pd.read_sql_query("SELECT * FROM historical_data ORDER BY timestamp DESC LIMIT 300", self.conn)
                  if len(df) < 10: return None, None, df
                  
                  iso = IsolationForest(contamination=0.05, random_state=42)
                  df['anomaly'] = iso.fit_predict(df[['cpu_load', 'latency']].fillna(0))
                  
                  reg = LinearRegression()
                  sub = df.head(50).iloc[::-1].reset_index(drop=True)
                  reg.fit(np.array(sub.index).reshape(-1, 1), sub['cpu_load'].values)
                  pred = reg.predict([[50]])[0]
                  
                  return df.iloc[0]['anomaly'], pred, df

          class DashboardGenerator:
              """GitHub Summaryìš© ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""
              
              @staticmethod
              def draw_bar(val, max_v=100):
                  # GitHub UIì—ì„œ ê¹”ë”í•˜ê²Œ ë³´ì´ë„ë¡ ë¬¸ì ë³€ê²½
                  l = int((val/max_v)*15)
                  return "â–“" * l + "â–‘" * (15-l)

              def generate_daily(self, load, traffic, anomaly, pred, df):
                  status_icon = "ğŸŸ¢" if anomaly == 1 else "ğŸ”´"
                  status_text = "Stable" if anomaly == 1 else "Critical"
                  
                  with open(DAILY_PATH, 'w', encoding='utf-8') as f:
                      f.write(f"### ğŸ“¡ Real-time Dashboard\n")
                      f.write(f"**Status:** {status_icon} {status_text} | **Time:** {datetime.datetime.now().strftime('%H:%M:%S')}\n\n")
                      
                      f.write(f"| Metric | Value | Visual |\n|---|---|---|\n")
                      f.write(f"| **CPU Load** | {load:.1f}% | `{self.draw_bar(load)}` |\n")
                      f.write(f"| **Traffic** | {traffic} | `{self.draw_bar(traffic, 2000)}` |\n")
                      f.write(f"| **AI Prediction** | {pred:.1f}% | (Next 10m) |\n\n")
                      
                      f.write("#### ğŸ“‰ Recent Traffic Logs\n")
                      f.write(df.head(5)[['timestamp', 'cpu_load', 'traffic']].to_markdown(index=False))

              def generate_weekly(self, conn):
                  df = pd.read_sql_query("SELECT * FROM historical_data WHERE timestamp >= date('now', '-7 days')", conn)
                  if df.empty: return
                  
                  with open(WEEKLY_PATH, 'w', encoding='utf-8') as f:
                      f.write(f"### ğŸ“° Weekly Intelligence\n")
                      f.write(f"**Data Points:** {len(df)}\n\n")
                      f.write(f"| Stat | Load | Traffic |\n|---|---|---|\n")
                      f.write(f"| **Avg** | {df['cpu_load'].mean():.1f}% | {df['traffic'].mean():.0f} |\n")
                      f.write(f"| **Max** | {df['cpu_load'].max():.1f}% | {df['traffic'].max()} |\n")

              def generate_audit(self, conn):
                  df = pd.read_sql_query("SELECT timestamp, actor, action, details FROM audit_trail ORDER BY id DESC LIMIT 5", conn)
                  if df.empty: return
                  
                  with open(AUDIT_PATH, 'w', encoding='utf-8') as f:
                      f.write(f"### ğŸ›¡ï¸ Security Audit (Last 5)\n")
                      f.write(df.to_markdown(index=False))

          def main():
              if not os.path.exists("data"): os.makedirs("data")
              
              conn = sqlite3.connect(DB_PATH)
              conn.execute('CREATE TABLE IF NOT EXISTS historical_data (id INTEGER PRIMARY KEY, cpu_load REAL, traffic INTEGER, latency REAL, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
              conn.execute('CREATE TABLE IF NOT EXISTS audit_trail (id INTEGER PRIMARY KEY, actor TEXT, action TEXT, details TEXT, data_hash TEXT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
              
              mgr = DataManager(conn)
              audit = AuditSystem(conn)
              ai = AIAnalyzer(conn)
              dash = DashboardGenerator()
              
              mgr.seed_data()
              load, traffic = mgr.collect()
              audit.log("System", "COLLECT", f"L:{load:.0f}")
              
              anom, pred, df = ai.analyze()
              
              # ë¦¬í¬íŠ¸ ìƒì„±
              dash.generate_daily(load, traffic, anom, pred, df)
              dash.generate_weekly(conn)
              dash.generate_audit(conn)
              
              conn.close()

          if __name__ == "__main__":
              main()
          EOF

      # 3. AI ì‹¤í–‰
      - name: ğŸš€ Run AI Engine
        run: python scripts/ai_dashboard_core.py

      # 4. [í•µì‹¬] í™”ë©´ì— ê²°ê³¼ ì¶œë ¥ (GitHub Job Summary)
      # ì´ ë‹¨ê³„ê°€ ì¶”ê°€ë˜ì–´ì•¼ ì‚¬ìš©ìê°€ í´ë¦­í–ˆì„ ë•Œ ë‚´ìš©ì´ ë°”ë¡œ ë³´ì…ë‹ˆë‹¤.
      - name: ğŸ“¢ Publish to Action Screen
        run: |
          echo "# ğŸš€ Grand Ops AI Dashboard v20" >> $GITHUB_STEP_SUMMARY
          echo "Run ID: ${{ github.run_id }} | Ref: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          
          # 1. ì¼ì¼ ë¦¬í¬íŠ¸ ì¶œë ¥
          if [ -f "$DAILY_REPORT" ]; then
            cat "$DAILY_REPORT" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # 2. ë³´ì•ˆ ê°ì‚¬ ì¶œë ¥
          if [ -f "$AUDIT_REPORT" ]; then
            cat "$AUDIT_REPORT" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # 3. ì£¼ê°„ ë¦¬í¬íŠ¸ (ìˆìœ¼ë©´ ì¶œë ¥)
          if [ -f "$WEEKLY_REPORT" ]; then
            cat "$WEEKLY_REPORT" >> $GITHUB_STEP_SUMMARY
          fi

      # 5. íŒŒì¼ ë°±ì—… ë° ë‹¤ìš´ë¡œë“œìš© ìœ ë¬¼ ì—…ë¡œë“œ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
      - name: ğŸ”’ Backup & Upload Artifacts
        run: |
          zip -e -P "$ARTIFACT_PASSWORD" dashboard_bundle.zip data/*.md data/*.db
      
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: AI-Dashboard-Bundle
          path: dashboard_bundle.zip

      # 6. Git ë™ê¸°í™”
      - name: ğŸ”„ Sync Reports
        run: |
          git config --global user.name "Dashboard-Bot"
          git config --global user.email "bot@grand-ops.ai"
          git config --global --add safe.directory $GITHUB_WORKSPACE
          
          [ -f "$DAILY_REPORT" ] && git add -f "$DAILY_REPORT"
          [ -f "$AUDIT_REPORT" ] && git add -f "$AUDIT_REPORT"
          [ -f "$DB_PATH" ] && git add -f "$DB_PATH"
          git add -A
          
          if [ -z "$(git status --porcelain)" ]; then exit 0; fi
          
          git commit -m "dashboard: ui update [skip ci]"
          
          COUNT=0
          while [ $COUNT -lt 5 ]; do
            git pull --rebase origin main || git rebase --abort
            git push origin main && exit 0
            sleep 5
            COUNT=$((COUNT+1))
          done
