name: ğŸ“Š Grand Ops AI-Reporting v19 (Multi-Report & Audit)

on:
  schedule:
    - cron: '*/10 * * * *' # 10ë¶„ ì£¼ê¸° ì‹¤í–‰
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: false

env:
  TZ: 'Asia/Seoul'
  # DB ê²½ë¡œ
  DB_PATH: 'data/grand_ops_server.db'
  # [New] 3ì¢… ë³´ê³ ì„œ ê²½ë¡œ ì •ì˜
  DAILY_REPORT: 'data/daily_insight.md'
  WEEKLY_REPORT: 'data/weekly_analysis.md'
  AUDIT_REPORT: 'data/security_audit.md'
  
  BACKUP_DIR: 'backup'
  ARTIFACT_PASSWORD: ${{ secrets.ARTIFACT_KEY || 'Report_Secure_2025' }}

permissions:
  contents: write

jobs:
  ai-reporting-server:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 1. í™˜ê²½ êµ¬ì„±
      - name: ğŸ—ï¸ Setup Reporting Environment
        run: |
          echo "ğŸ§¬ Initializing AI & Reporting Stack..."
          mkdir -p data backup scripts
          python -m pip install --upgrade pip
          echo -e "pandas\nnumpy\nscikit-learn\nscipy\ntabulate" > requirements.txt
          pip install -r requirements.txt

      # 2. [Core] ë¦¬í¬íŒ… ì—”ì§„ì´ íƒ‘ì¬ëœ í†µí•© ìŠ¤í¬ë¦½íŠ¸ (v19.0)
      - name: ğŸ§  Generate Reporting Engine Script
        run: |
          cat <<'EOF' > scripts/ai_report_core.py
          import sqlite3
          import os
          import datetime
          import hashlib
          import sys
          import pandas as pd
          import numpy as np
          from sklearn.ensemble import IsolationForest
          from sklearn.linear_model import LinearRegression

          # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
          DB_PATH = os.getenv('DB_PATH', 'data/grand_ops_server.db')
          DAILY_PATH = os.getenv('DAILY_REPORT', 'data/daily_insight.md')
          WEEKLY_PATH = os.getenv('WEEKLY_REPORT', 'data/weekly_analysis.md')
          AUDIT_PATH = os.getenv('AUDIT_REPORT', 'data/security_audit.md')

          class AuditSystem:
              """ë³´ì•ˆ ê°ì‚¬ ë¡œê·¸ ë° ë¬´ê²°ì„± ê´€ë¦¬"""
              def __init__(self, conn):
                  self.conn = conn
              
              def log(self, actor, action, details):
                  try:
                      raw = f"{actor}{action}{details}{datetime.datetime.now()}"
                      h_val = hashlib.sha256(raw.encode()).hexdigest()
                      self.conn.execute(
                          'INSERT INTO audit_trail (actor, action, details, data_hash) VALUES (?,?,?,?)', 
                          (actor, action, details, h_val)
                      )
                      self.conn.commit()
                  except Exception as e:
                      print(f"âš ï¸ Audit Error: {e}")

          class DataManager:
              """ë°ì´í„° ì‹œë”© ë° ìˆ˜ì§‘"""
              def __init__(self, conn):
                  self.conn = conn

              def seed_data(self):
                  """ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ì´ˆê¸°í™” (Cold Start ë°©ì§€)"""
                  cursor = self.conn.cursor()
                  cursor.execute("SELECT count(*) FROM historical_data")
                  if cursor.fetchone()[0] < 10:
                      print("ğŸŒ± Seeding dummy data...")
                      data = []
                      for i in range(50):
                          data.append((
                              np.random.normal(40, 10), 
                              np.random.randint(500, 1500), 
                              np.random.normal(20, 5),
                              (datetime.datetime.now() - datetime.timedelta(minutes=10*i))
                          ))
                      cursor.executemany('INSERT INTO historical_data (cpu_load, traffic, latency, timestamp) VALUES (?,?,?,?)', data)
                      self.conn.commit()

              def collect(self):
                  load = max(0, np.random.normal(55, 12))
                  traffic = max(0, int(np.random.poisson(1100)))
                  latency = max(0, np.random.exponential(18))
                  self.conn.execute('INSERT INTO historical_data (cpu_load, traffic, latency) VALUES (?,?,?)', (load, traffic, latency))
                  self.conn.commit()
                  return load, traffic

          class AIAnalyzer:
              """AI ë¶„ì„ ë° ì˜ˆì¸¡"""
              def __init__(self, conn):
                  self.conn = conn
              
              def analyze(self):
                  df = pd.read_sql_query("SELECT * FROM historical_data ORDER BY timestamp DESC LIMIT 300", self.conn)
                  if len(df) < 10: return None, None, df
                  
                  # ì´ìƒ íƒì§€
                  iso = IsolationForest(contamination=0.05, random_state=42)
                  df['anomaly'] = iso.fit_predict(df[['cpu_load', 'latency']].fillna(0))
                  
                  # ì˜ˆì¸¡ (ìµœê·¼ ë°ì´í„° ê¸°ì¤€)
                  reg = LinearRegression()
                  sub = df.head(50).iloc[::-1].reset_index(drop=True)
                  reg.fit(np.array(sub.index).reshape(-1, 1), sub['cpu_load'].values)
                  pred = reg.predict([[50]])[0]
                  
                  return df.iloc[0]['anomaly'], pred, df

          class MDReportGenerator:
              """[New] Markdown ì „ë¬¸ ë³´ê³ ì„œ ìƒì„±ê¸°"""
              
              @staticmethod
              def draw_bar(val, max_v=100):
                  l = int((val/max_v)*20)
                  return "â–ˆ" * l + "â–‘" * (20-l)

              def generate_daily(self, load, traffic, anomaly, pred, df):
                  status = "âœ… NORMAL" if anomaly == 1 else "ğŸš¨ CRITICAL"
                  with open(DAILY_PATH, 'w', encoding='utf-8') as f:
                      f.write(f"# ğŸ“… Daily Ops Insight\n")
                      f.write(f"**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
                      f.write(f"## ğŸš¦ System Status: {status}\n")
                      f.write(f"- **CPU Load:** `{self.draw_bar(load)}` ({load:.1f}%)\n")
                      f.write(f"- **Traffic:** `{self.draw_bar(traffic, 2000)}` ({traffic} hits)\n")
                      f.write(f"- **AI Prediction (Next 10m):** {pred:.1f}% Load\n\n")
                      
                      f.write("### ğŸ“‰ Recent Logs\n")
                      f.write(df.head(5)[['timestamp', 'cpu_load', 'traffic']].to_markdown(index=False))

              def generate_weekly(self, conn):
                  # ì£¼ê°„ ë°ì´í„° ì§‘ê³„
                  df = pd.read_sql_query("SELECT * FROM historical_data WHERE timestamp >= date('now', '-7 days')", conn)
                  if df.empty: return
                  
                  with open(WEEKLY_PATH, 'w', encoding='utf-8') as f:
                      f.write(f"# ğŸ“° Weekly AI Analysis\n")
                      f.write(f"**Period:** Last 7 Days\n\n")
                      f.write(f"## ğŸ“Š Key Statistics\n")
                      f.write(f"| Metric | Mean | Max | Min | StdDev |\n|---|---|---|---|---|\n")
                      f.write(f"| CPU Load | {df['cpu_load'].mean():.1f}% | {df['cpu_load'].max():.1f}% | {df['cpu_load'].min():.1f}% | {df['cpu_load'].std():.1f} |\n")
                      f.write(f"| Traffic | {df['traffic'].mean():.0f} | {df['traffic'].max()} | {df['traffic'].min()} | {df['traffic'].std():.0f} |\n")
                      f.write(f"\n## ğŸ“ˆ Growth Trend\n")
                      f.write(f"Total data points processed: **{len(df)}**\n")

              def generate_audit(self, conn):
                  # ë³´ì•ˆ ê°ì‚¬ ë¡œê·¸ ë¦¬í¬íŠ¸
                  df = pd.read_sql_query("SELECT timestamp, actor, action, details, data_hash FROM audit_trail ORDER BY id DESC LIMIT 20", conn)
                  if df.empty: return
                  
                  with open(AUDIT_PATH, 'w', encoding='utf-8') as f:
                      f.write(f"# ğŸ›¡ï¸ Security Audit Log\n")
                      f.write(f"**Verified By:** AI-Guardian System\n\n")
                      f.write(f"## ğŸ•µï¸ Recent Activities\n")
                      # HashëŠ” ë„ˆë¬´ ê¸°ë‹ˆê¹Œ ì• 8ìë¦¬ë§Œ ë…¸ì¶œ
                      df['data_hash'] = df['data_hash'].apply(lambda x: x[:8] + '...')
                      f.write(df.to_markdown(index=False))

          def main():
              print("ğŸš€ Starting Reporting Engine v19...")
              if not os.path.exists("data"): os.makedirs("data")
              
              conn = sqlite3.connect(DB_PATH)
              conn.execute('CREATE TABLE IF NOT EXISTS historical_data (id INTEGER PRIMARY KEY, cpu_load REAL, traffic INTEGER, latency REAL, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
              conn.execute('CREATE TABLE IF NOT EXISTS audit_trail (id INTEGER PRIMARY KEY, actor TEXT, action TEXT, details TEXT, data_hash TEXT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
              
              # Modules
              data_mgr = DataManager(conn)
              audit = AuditSystem(conn)
              ai = AIAnalyzer(conn)
              reporter = MDReportGenerator()
              
              # 1. Process Data
              data_mgr.seed_data()
              load, traffic = data_mgr.collect()
              audit.log("System", "COLLECT", f"L:{load:.0f}")
              
              # 2. AI Analysis
              anom, pred, df = ai.analyze()
              
              # 3. Generate Reports
              print("ğŸ“ Generating Daily Report...")
              reporter.generate_daily(load, traffic, anom, pred, df)
              
              print("ğŸ“ Generating Weekly Report...")
              reporter.generate_weekly(conn)
              
              print("ğŸ“ Generating Audit Report...")
              audit.log("Reporter", "GENERATE", "MD Files Updated")
              reporter.generate_audit(conn)
              
              conn.close()
              print("âœ… All Reports Generated Successfully.")

          if __name__ == "__main__":
              main()
          EOF

      # 3. ì—”ì§„ ì‹¤í–‰
      - name: ğŸš€ Run AI & Reporting
        run: |
          python scripts/ai_report_core.py
          
          echo "ğŸ“„ Checking Outputs..."
          ls -l data/*.md

      # 4. ë³´ì•ˆ ë°±ì—…
      - name: ğŸ”’ Encrypt Artifacts
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          cp "$DB_PATH" "$BACKUP_DIR/report_db_$TIMESTAMP.bak"
          zip -e -P "$ARTIFACT_PASSWORD" secure_reports.zip data/*.md data/*.db backup/*
          
          cd "$BACKUP_DIR" && ls -t *.bak | tail -n +6 | xargs -r rm --

      - name: Upload Secured Data
        uses: actions/upload-artifact@v4
        with:
          name: ai-reports-bundle
          path: secure_reports.zip
          retention-days: 1

      # 5. Git ë™ê¸°í™” (3ì¢… ë³´ê³ ì„œ ëª¨ë‘ ë°˜ì˜)
      - name: ğŸ”„ Sync Reports
        run: |
          git config --global user.name "AI-Reporter-Bot"
          git config --global user.email "reporter@grand-ops.ai"
          git config --global --add safe.directory $GITHUB_WORKSPACE

          echo "ğŸ“¦ Staging Reports..."
          
          # 3ì¢… ë³´ê³ ì„œ ì¡´ì¬ í™•ì¸ í›„ Add
          [ -f "$DAILY_REPORT" ] && git add -f "$DAILY_REPORT"
          [ -f "$WEEKLY_REPORT" ] && git add -f "$WEEKLY_REPORT"
          [ -f "$AUDIT_REPORT" ] && git add -f "$AUDIT_REPORT"
          [ -f "$DB_PATH" ] && git add -f "$DB_PATH"
          
          git add -A

          if [ -z "$(git status --porcelain)" ]; then
             echo "âœ… Reports are up to date."
             exit 0
          fi

          echo "ğŸ’¾ Committing New Insights..."
          git commit -m "report: daily/weekly/audit update [skip ci]"

          # Retry Loop
          MAX_RETRIES=5
          COUNT=0
          while [ $COUNT -lt $MAX_RETRIES ]; do
            git pull --rebase origin main || git rebase --abort
            if git push origin main; then
              echo "ğŸš€ Synced Successfully!"
              exit 0
            fi
            sleep 5
            COUNT=$((COUNT+1))
          done
          exit 1
