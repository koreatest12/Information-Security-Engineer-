name: üß† Grand Ops GenAI-Copilot v26 (Search & RAG)

on:
  schedule:
    - cron: '*/30 * * * *' # 30Î∂ÑÎßàÎã§ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è ÌïôÏäµ
  workflow_dispatch:
    inputs:
      copilot_query:
        description: 'üí¨ Ask Copilot (e.g., "Any security threats?", "BTC trend?")'
        required: false
        default: 'Summarize the current system status and major news.'
      create_release:
        description: 'Publish Release?'
        required: false
        type: boolean
        default: false

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: false

env:
  TZ: 'Asia/Seoul'
  DB_PATH: 'data/grand_ops_universe.db'
  DASHBOARD_FILE: 'data/omni_dashboard.md'
  RELEASE_DIR: 'dist'
  # ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ÏùÑ ÌôòÍ≤Ω Î≥ÄÏàòÎ°ú Ï†ÑÎã¨
  USER_QUERY: ${{ inputs.copilot_query || 'Summarize system status' }}
  ARTIFACT_PASSWORD: ${{ secrets.ARTIFACT_KEY || 'GenAI_Secure_2025' }}

permissions:
  contents: write

jobs:
  genai-intelligence-engine:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: üèóÔ∏è Initialize GenAI Stack
        run: |
          echo "üß† Booting GenAI Neural Engine..."
          mkdir -p data backup scripts dist
          python -m pip install --upgrade pip
          # [ÌïµÏã¨] scikit-learn(Î≤°ÌÑ∞Í≤ÄÏÉâ), textblob(NLP), feedparser(Îâ¥Ïä§)
          echo -e "requests\npandas\nnumpy\nscikit-learn\ntabulate\npsutil\nfeedparser\ntextblob" > requirements.txt
          pip install -r requirements.txt
          python -m textblob.download_corpora

      - name: üß† Generate GenAI Core Script
        run: |
          cat <<'EOF' > scripts/genai_core.py
          import sqlite3
          import os
          import datetime
          import requests
          import psutil
          import feedparser
          import pandas as pd
          import numpy as np
          from textblob import TextBlob
          from sklearn.feature_extraction.text import TfidfVectorizer
          from sklearn.metrics.pairwise import cosine_similarity

          # ÏÑ§Ï†ï
          DB_PATH = os.getenv('DB_PATH', 'data/grand_ops_universe.db')
          DASH_PATH = os.getenv('DASHBOARD_FILE', 'data/omni_dashboard.md')
          USER_QUERY = os.getenv('USER_QUERY', '')

          class KnowledgeBase:
              """üìö RAG(Í≤ÄÏÉâ Ï¶ùÍ∞ï ÏÉùÏÑ±)Î•º ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è Ï†ÑÏ≤òÎ¶¨"""
              def __init__(self, conn):
                  self.conn = conn
                  self.documents = [] # AIÍ∞Ä ÌïôÏäµÌï† ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞
                  self.metadata = []  # Ï∂úÏ≤ò Ï†ïÎ≥¥

              def ingest_data(self):
                  # 1. ÏãúÏä§ÌÖú Î°úÍ∑∏ ÌïôÏäµ
                  df_sys = pd.read_sql_query("SELECT * FROM system_logs ORDER BY id DESC LIMIT 50", self.conn)
                  if not df_sys.empty:
                      avg_cpu = df_sys['cpu_usage'].mean()
                      self.documents.append(f"System CPU load average is {avg_cpu:.1f}%. Recent trend shows stability.")
                      self.metadata.append("System Metrics")
                      
                      last_btc = df_sys.iloc[0]['btc_price']
                      self.documents.append(f"Bitcoin price is currently ${last_btc:,.2f}.")
                      self.metadata.append("Market Data")

                  # 2. Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ ÌïôÏäµ (RSS)
                  feeds = [
                      "https://feeds.feedburner.com/TheHackersNews",
                      "https://techcrunch.com/category/artificial-intelligence/feed/"
                  ]
                  for url in feeds:
                      try:
                          f = feedparser.parse(url)
                          for e in f.entries[:3]:
                              self.documents.append(f"News: {e.title}")
                              self.metadata.append(f"Source: {f.feed.title}")
                      except: pass

              def get_corpus(self):
                  return self.documents, self.metadata

          class NeuralSearchEngine:
              """üß† Î≤°ÌÑ∞ Í∏∞Î∞ò ÏùòÎØ∏ Í≤ÄÏÉâ (Semantic Search)"""
              def __init__(self, documents):
                  self.documents = documents
                  # TF-IDF Î≤°ÌÑ∞Ìôî (ÌÖçÏä§Ìä∏Î•º Ïà´ÏûêÎ°ú Î≥ÄÌôò)
                  self.vectorizer = TfidfVectorizer(stop_words='english')
                  if documents:
                      self.tfidf_matrix = self.vectorizer.fit_transform(documents)
                  else:
                      self.tfidf_matrix = None

              def search(self, query, top_k=3):
                  if self.tfidf_matrix is None: return []
                  
                  # ÏßàÎ¨∏ÏùÑ Î≤°ÌÑ∞Î°ú Î≥ÄÌôò
                  query_vec = self.vectorizer.transform([query])
                  # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                  cosine_similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
                  # ÏÉÅÏúÑ KÍ∞ú Ï∂îÏ∂ú
                  related_docs_indices = cosine_similarities.argsort()[:-top_k:-1]
                  
                  results = []
                  for i in related_docs_indices:
                      if cosine_similarities[i] > 0.1: # Ïú†ÏÇ¨ÎèÑÍ∞Ä ÏùºÏ†ï ÏàòÏ§Ä Ïù¥ÏÉÅÏùº ÎïåÎßå
                          results.append(self.documents[i])
                  return results

          class GenAICopilot:
              def __init__(self, conn):
                  self.conn = conn
                  self.kb = KnowledgeBase(conn)
                  self.kb.ingest_data()
                  docs, _ = self.kb.get_corpus()
                  self.engine = NeuralSearchEngine(docs)

              def generate_response(self, user_query):
                  if not user_query: return "No query provided."
                  
                  # 1. ÏßÄÏãù Í≤ÄÏÉâ (Retrieval)
                  context = self.engine.search(user_query)
                  
                  # 2. ÎãµÎ≥Ä ÏÉùÏÑ± (Generation - Rule based Wrapper)
                  timestamp = datetime.datetime.now().strftime("%H:%M")
                  response = f"### ü§ñ Copilot Answer (at {timestamp})\n"
                  response += f"> **Q:** \"{user_query}\"\n\n"
                  
                  if context:
                      response += "**‚úÖ I found relevant information in your data:**\n"
                      for item in context:
                          response += f"- {item}\n"
                      
                      # Í∞êÏ†ï Î∂ÑÏÑù Ï∂îÍ∞Ä
                      blob = TextBlob(user_query)
                      if blob.sentiment.polarity < 0:
                          response += "\n*‚ö†Ô∏è You seem concerned. I recommend checking the logs in detail.*"
                  else:
                      response += "‚ùå I couldn't find specific data matching your query in the current logs/news."
                      
                  return response

          class DashboardGenerator:
              def generate(self, copilot_response, sys_stats):
                  with open(DASH_PATH, 'w', encoding='utf-8') as f:
                      f.write(f"# üß† Grand Ops GenAI-Copilot Dashboard\n")
                      f.write(f"> **System Time:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

                      # 1. Copilot ÏÑπÏÖò (ÏµúÏÉÅÎã® Î∞∞Ïπò)
                      f.write(f"{copilot_response}\n\n")
                      f.write("---\n")

                      # 2. ÏãúÏä§ÌÖú ÏöîÏïΩ
                      cpu, ram, _ = sys_stats
                      f.write("### üñ•Ô∏è Live System Metrics\n")
                      f.write(f"- **CPU:** {cpu}% | **RAM:** {ram}%\n")
                      
                      # 3. Îç∞Ïù¥ÌÑ∞ ÎàÑÏ†Å ÏïàÎÇ¥
                      f.write("\n> *All data is being vectorized and indexed for future searches.*\n")

          def main():
              if not os.path.exists("data"): os.makedirs("data")
              conn = sqlite3.connect(DB_PATH)
              conn.execute('CREATE TABLE IF NOT EXISTS system_logs (id INTEGER PRIMARY KEY, cpu_usage REAL, btc_price REAL, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
              
              # Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è Ï†ÄÏû•
              cpu = psutil.cpu_percent()
              try:
                  btc = requests.get("https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd").json()['bitcoin']['usd']
              except: btc = 0
              
              conn.execute("INSERT INTO system_logs (cpu_usage, btc_price) VALUES (?, ?)", (cpu, btc))
              conn.commit()

              # Copilot Ïã§Ìñâ
              copilot = GenAICopilot(conn)
              ai_answer = copilot.generate_response(USER_QUERY)
              
              # ÎåÄÏãúÎ≥¥Îìú ÏÉùÏÑ±
              dash = DashboardGenerator()
              dash.generate(ai_answer, (cpu, psutil.virtual_memory().percent, 0))
              
              conn.close()
              print("‚úÖ GenAI-Copilot Task Finished.")

          if __name__ == "__main__":
              main()
          EOF

      - name: üöÄ Run GenAI-Copilot
        run: python scripts/genai_core.py

      - name: üì¢ Publish Copilot Response
        run: cat "$DASHBOARD_FILE" >> $GITHUB_STEP_SUMMARY

      # Îç∞Ïù¥ÌÑ∞ Î∞±ÏóÖ
      - name: üîí Encrypt & Sync
        run: |
          zip -e -P "$ARTIFACT_PASSWORD" genai_backup.zip data/*.md data/*.db
          
          git config --global user.name "Copilot-Bot"
          git config --global user.email "copilot@grand-ops.ai"
          git config --global --add safe.directory $GITHUB_WORKSPACE
          
          [ -f "$DASHBOARD_FILE" ] && git add -f "$DASHBOARD_FILE"
          [ -f "$DB_PATH" ] && git add -f "$DB_PATH"
          git add -A
          
          if [ -z "$(git status --porcelain)" ]; then exit 0; fi
          git commit -m "genai: copilot query response & indexing [skip ci]"
          git push origin main || echo "Sync skipped"
