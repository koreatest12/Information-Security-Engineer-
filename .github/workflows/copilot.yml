name: ğŸŒ Grand Ops AI-Server v18 (Stable, Visual & Seed)

on:
  schedule:
    - cron: '*/10 * * * *' # 10ë¶„ ì£¼ê¸°
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: false

env:
  TZ: 'Asia/Seoul'
  DB_PATH: 'data/grand_ops_server.db'
  DAILY_REPORT: 'data/daily_insight.md'
  WEEKLY_NEWS: 'data/weekly_ai_news.md'
  BACKUP_DIR: 'backup'
  ARTIFACT_PASSWORD: ${{ secrets.ARTIFACT_KEY || 'HyperAI_2025_Key' }}

permissions:
  contents: write

jobs:
  ai-hyper-server:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 1. í™˜ê²½ ì„¤ì • ë° ë””ë ‰í† ë¦¬ ë³µêµ¬
      - name: ğŸ—ï¸ Setup Stable Environment
        run: |
          echo "ğŸ§¬ Initializing Stable AI Environment..."
          mkdir -p data backup scripts
          
          python -m pip install --upgrade pip
          
          # [í•µì‹¬] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (scikit-learn í•„ìˆ˜)
          echo -e "pandas\nnumpy\nscikit-learn\nscipy" > requirements.txt
          pip install -r requirements.txt
          echo "âœ… Environment Ready."

      # 2. [ìˆ˜ì •ë¨] ì•ˆì •ì„±ì´ ê°•í™”ëœ AI ì½”ì–´ ìŠ¤í¬ë¦½íŠ¸ (v18.0)
      - name: ğŸ§  Generate Robust AI Script
        run: |
          cat <<'EOF' > scripts/ai_server_core.py
          import sqlite3
          import os
          import datetime
          import random
          import hashlib
          import sys
          import pandas as pd
          import numpy as np
          from sklearn.ensemble import IsolationForest
          from sklearn.linear_model import LinearRegression

          # í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
          DB_PATH = os.getenv('DB_PATH', 'data/grand_ops_server.db')
          DAILY_REPORT = os.getenv('DAILY_REPORT', 'data/daily_insight.md')
          WEEKLY_NEWS = os.getenv('WEEKLY_NEWS', 'data/weekly_ai_news.md')

          class AuditLogger:
              def __init__(self, conn):
                  self.conn = conn
              
              def log(self, actor, action, details):
                  try:
                      raw = f"{actor}{action}{details}{datetime.datetime.now()}"
                      h_val = hashlib.sha256(raw.encode()).hexdigest()
                      self.conn.execute('INSERT INTO audit_trail (actor, action, details, data_hash) VALUES (?,?,?,?)', 
                                        (actor, action, details, h_val))
                      self.conn.commit()
                  except Exception as e:
                      print(f"âš ï¸ Audit Log Error: {e}")

          class DataHandler:
              def __init__(self, conn):
                  self.conn = conn

              def seed_initial_data(self):
                  """[Fix] ì´ˆê¸° ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì—ëŸ¬ ë°©ì§€ (Seeding)"""
                  cursor = self.conn.cursor()
                  cursor.execute("SELECT count(*) FROM historical_data")
                  cnt = cursor.fetchone()[0]
                  
                  if cnt < 10:
                      print("ğŸŒ± Seeding initial data for AI training...")
                      data = []
                      for i in range(50):
                          load = np.random.normal(40, 10)
                          traf = np.random.randint(500, 1500)
                          lat = np.random.normal(20, 5)
                          # ê³¼ê±° ì‹œê°„ìœ¼ë¡œ ìƒì„±
                          past_time = datetime.datetime.now() - datetime.timedelta(minutes=10*i)
                          data.append((load, traf, lat, past_time))
                      
                      cursor.executemany('INSERT INTO historical_data (cpu_load, traffic, latency, timestamp) VALUES (?,?,?,?)', data)
                      self.conn.commit()
                      print("âœ… Seeding Completed.")

              def collect(self):
                  load = max(0, np.random.normal(50, 15))
                  traffic = max(0, int(np.random.poisson(1000)))
                  latency = max(0, np.random.exponential(20))
                  
                  self.conn.execute('INSERT INTO historical_data (cpu_load, traffic, latency) VALUES (?,?,?)', 
                                    (load, traffic, latency))
                  self.conn.commit()
                  return load, traffic, latency

          class AIEngine:
              def __init__(self, conn):
                  self.conn = conn

              def analyze(self):
                  # ë°ì´í„° ë¡œë“œ
                  df = pd.read_sql_query("SELECT * FROM historical_data ORDER BY timestamp DESC LIMIT 200", self.conn)
                  
                  # [Fix] ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë¶„ì„ ìŠ¤í‚µ (None ë°˜í™˜)
                  if len(df) < 10:
                      return None, None, df

                  # 1. ì´ìƒ íƒì§€
                  try:
                      iso = IsolationForest(contamination=0.05, random_state=42)
                      # í•™ìŠµìš© ë°ì´í„° (NaN ì œê±°)
                      X = df[['cpu_load', 'latency']].fillna(0)
                      df['anomaly'] = iso.fit_predict(X)
                      current_anomaly = df.iloc[0]['anomaly'] # ìµœì‹  ë°ì´í„°
                  except Exception as e:
                      print(f"âš ï¸ AI Model Error: {e}")
                      current_anomaly = 1 # ê¸°ë³¸ê°’ ì •ìƒ

                  # 2. ì„ í˜• ì˜ˆì¸¡
                  try:
                      reg = LinearRegression()
                      # ìµœê·¼ 50ê°œë§Œ ì‚¬ìš©
                      sub_df = df.head(50).iloc[::-1].reset_index(drop=True) # ì‹œê°„ìˆœ ì •ë ¬
                      X_reg = np.array(sub_df.index).reshape(-1, 1)
                      y_reg = sub_df['cpu_load'].values
                      reg.fit(X_reg, y_reg)
                      next_pred = reg.predict([[50]])[0]
                  except Exception:
                      next_pred = None

                  return current_anomaly, next_pred, df

          class Visualizer:
              """ASCII ì°¨íŠ¸ ìƒì„±ê¸°"""
              @staticmethod
              def draw_ascii_bar(val, max_val=100):
                  length = int((val / max_val) * 20)
                  return "â–ˆ" * length + "â–‘" * (20 - length)

          def main():
              print("ğŸš€ AI Server Core v18.0 Starting...")
              if not os.path.exists("data"): os.makedirs("data")

              conn = sqlite3.connect(DB_PATH)
              
              # í…Œì´ë¸” ìƒì„±
              conn.execute('CREATE TABLE IF NOT EXISTS historical_data (id INTEGER PRIMARY KEY, cpu_load REAL, traffic INTEGER, latency REAL, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
              conn.execute('CREATE TABLE IF NOT EXISTS audit_trail (id INTEGER PRIMARY KEY, actor TEXT, action TEXT, details TEXT, data_hash TEXT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
              
              # ëª¨ë“ˆ ì´ˆê¸°í™”
              handler = DataHandler(conn)
              engine = AIEngine(conn)
              audit = AuditLogger(conn)

              # 1. ë°ì´í„° ì‹œë”© ë° ìˆ˜ì§‘
              handler.seed_initial_data() # [ì¤‘ìš”]
              load, traf, lat = handler.collect()
              audit.log("AI_Agent", "COLLECT", f"L:{load:.1f}, T:{traf}")

              # 2. AI ë¶„ì„
              anomaly, pred, df = engine.analyze()
              
              # [Fix] í¬ë§·íŒ… ì•ˆì „ ì²˜ë¦¬
              status_str = "âœ… STABLE" if anomaly == 1 else "ğŸš¨ ABNORMAL"
              pred_str = f"{pred:.2f}%" if pred is not None else "Analyzing..."
              
              # 3. ë¦¬í¬íŠ¸ ì‘ì„± (Visual ASCII Chart í¬í•¨)
              with open(DAILY_REPORT, 'w', encoding='utf-8') as f:
                  f.write(f"# ğŸ“Š Grand Ops Daily Insight\n")
                  f.write(f"**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                  
                  f.write("## ğŸ“¡ System Dashboard\n")
                  f.write(f"- **Current Load:** `{Visualizer.draw_ascii_bar(load)}` ({load:.1f}%)\n")
                  f.write(f"- **Traffic Vol:** `{Visualizer.draw_ascii_bar(traf, 2000)}` ({traf} reqs)\n")
                  f.write(f"- **Latency:** {lat:.2f} ms\n\n")
                  
                  f.write("## ğŸ§  AI Diagnosis\n")
                  f.write(f"- **System Health:** {status_str}\n")
                  f.write(f"- **Load Prediction (Next 10m):** {pred_str}\n")
                  
                  if df is not None and not df.empty:
                      f.write("\n## ğŸ“‰ Recent Trend (Last 5 Entries)\n")
                      f.write("| Time | Load | Traffic | Status |\n|---|---|---|---|\n")
                      for _, row in df.head(5).iterrows():
                          ts = row.get('timestamp', 'N/A')
                          ld = row['cpu_load']
                          tr = row['traffic']
                          st = "ğŸ”´" if row.get('anomaly', 1) == -1 else "ğŸŸ¢"
                          f.write(f"| {ts} | {ld:.1f}% | {tr} | {st} |\n")

              print("âœ… Core Logic Finished Successfully.")
              conn.close()

          if __name__ == "__main__":
              try:
                  main()
              except Exception as e:
                  print(f"âŒ Critical Error: {e}")
                  sys.exit(1) # ì—ëŸ¬ ë°œìƒ ì‹œ ëª…í™•íˆ ì•Œë¦¼
          EOF

      # 3. ì‹¤í–‰ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)
      - name: ğŸš€ Run AI Hyper-Server
        run: |
          python scripts/ai_server_core.py
          
          echo "ğŸ“„ Report Preview:"
          cat data/daily_insight.md
          
          # íŒŒì¼ì´ ìˆì„ ë•Œë§Œ ë‰´ìŠ¤ ì¶œë ¥
          if [ -f "data/weekly_ai_news.md" ]; then
            echo "ğŸ“° WEEKLY NEWS:"
            cat data/weekly_ai_news.md
          fi

      # 4. ë°ì´í„° ë³´í˜¸ ë° ë°±ì—…
      - name: ğŸ”’ Encrypt & Backup
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          cp "$DB_PATH" "$BACKUP_DIR/server_$TIMESTAMP.bak"
          zip -e -P "$ARTIFACT_PASSWORD" ai_server_data.zip data/* backup/*
          # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
          cd "$BACKUP_DIR" && ls -t *.bak | tail -n +6 | xargs -r rm --

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: grand-ops-data
          path: ai_server_data.zip
          retention-days: 1

      # 5. ì•ˆì „í•œ ë™ê¸°í™” (Force Sync Fix)
      - name: ğŸ”„ Sync Data State
        run: |
          git config --global user.name "AI-Server-Bot"
          git config --global user.email "server@grand-ops.ai"
          git config --global --add safe.directory $GITHUB_WORKSPACE

          echo "ğŸ“¦ Staging Data..."
          
          # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ Add (ì—ëŸ¬ ë°©ì§€)
          [ -f "data/grand_ops_server.db" ] && git add -f data/grand_ops_server.db
          [ -f "data/daily_insight.md" ] && git add -f data/daily_insight.md
          [ -f "data/weekly_ai_news.md" ] && git add -f data/weekly_ai_news.md
          
          git add -A

          if [ -z "$(git status --porcelain)" ]; then
             echo "âœ… Nothing to sync."
             exit 0
          fi

          echo "ğŸ’¾ Committing..."
          git commit -m "ai: system update & visual report [skip ci]"

          MAX_RETRIES=5
          COUNT=0
          while [ $COUNT -lt $MAX_RETRIES ]; do
            git pull --rebase origin main || git rebase --abort
            if git push origin main; then
              echo "ğŸš€ Sync Success!"
              exit 0
            fi
            sleep 5
            COUNT=$((COUNT+1))
          done
          exit 1
