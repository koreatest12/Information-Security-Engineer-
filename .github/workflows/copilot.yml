name: ğŸŒ Grand Ops AI-Server v17 (News, Audit & Accumulation)

on:
  schedule:
    - cron: '*/10 * * * *' # 10ë¶„ë§ˆë‹¤ ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµ
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: false

env:
  TZ: 'Asia/Seoul'
  DB_PATH: 'data/grand_ops_server.db'
  DAILY_REPORT: 'data/daily_insight.md'
  WEEKLY_NEWS: 'data/weekly_ai_news.md'
  BACKUP_DIR: 'backup'
  ARTIFACT_PASSWORD: ${{ secrets.ARTIFACT_KEY || 'HyperAI_2025_Key' }}

permissions:
  contents: write

jobs:
  # AI ì„œë²„ êµ¬ë™ ë° ì§€ëŠ¥í˜• ë¶„ì„ Job
  ai-hyper-server:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 1. ê³ ì„±ëŠ¥ AI/ë°ì´í„° í™˜ê²½ êµ¬ì¶•
      - name: ğŸ—ï¸ Setup Hyper-Intelligence Server
        run: |
          echo "ğŸ§¬ Initializing AI Server Environment..."
          mkdir -p data backup scripts
          
          python -m pip install --upgrade pip
          
          # [í•µì‹¬] ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, í†µê³„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ê±° íƒ‘ì¬
          echo "ğŸ“¦ Installing Advanced AI Stack..."
          echo -e "pandas\nnumpy\nscikit-learn\nscipy\nrequests" > requirements.txt
          pip install -r requirements.txt

      # 2. [Ultimate] AI ì„œë²„ í†µí•© ìŠ¤í¬ë¦½íŠ¸ (ë‰´ìŠ¤, ì˜ˆì¸¡, ê°ì‚¬, ëˆ„ì  í¬í•¨)
      - name: ğŸ§  Generate AI Core Script
        run: |
          cat <<'EOF' > scripts/ai_server_core.py
          import sqlite3
          import os
          import datetime
          import random
          import hashlib
          import pandas as pd
          import numpy as np
          from sklearn.ensemble import IsolationForest
          from sklearn.linear_model import LinearRegression

          # í™˜ê²½ ë³€ìˆ˜
          DB_PATH = os.getenv('DB_PATH', 'data/grand_ops_server.db')
          DAILY_REPORT = os.getenv('DAILY_REPORT', 'data/daily_insight.md')
          WEEKLY_NEWS = os.getenv('WEEKLY_NEWS', 'data/weekly_ai_news.md')

          class AuditLogger:
              """ë³´ì•ˆ ê°ì‚¬(Audit) ë° ë¬´ê²°ì„± ê²€ì¦"""
              def __init__(self, conn):
                  self.conn = conn
              
              def log_action(self, actor, action, details):
                  # ìœ„ë³€ì¡° ë°©ì§€ë¥¼ ìœ„í•œ ë°ì´í„° í•´ì‹œ ìƒì„±
                  raw_data = f"{actor}{action}{details}{datetime.datetime.now()}"
                  hash_val = hashlib.sha256(raw_data.encode()).hexdigest()
                  
                  cursor = self.conn.cursor()
                  cursor.execute('''
                      INSERT INTO audit_trail (actor, action, details, data_hash)
                      VALUES (?, ?, ?, ?)
                  ''', (actor, action, details, hash_val))
                  self.conn.commit()

          class DataCollector:
              """ë°ì´í„° ìˆ˜ì§‘ ë° ì˜êµ¬ ëˆ„ì """
              def __init__(self, conn):
                  self.conn = conn
                  
              def collect_and_accumulate(self):
                  # (ì‹œë®¬ë ˆì´ì…˜) ì™¸ë¶€ ì„œë²„/ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘
                  # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” API í˜¸ì¶œë¡œ ëŒ€ì²´ ê°€ëŠ¥
                  load = np.random.normal(50, 15)  # ì„œë²„ ë¶€í•˜
                  traffic = np.random.poisson(1000) # íŠ¸ë˜í”½
                  latency = np.random.exponential(20) # ì§€ì—° ì‹œê°„
                  
                  cursor = self.conn.cursor()
                  # [í•µì‹¬] ë°ì´í„°ë¥¼ ë®ì–´ì“°ì§€ ì•Šê³  ê³„ì† ëˆ„ì  (Accumulation)
                  cursor.execute('''
                      INSERT INTO historical_data (cpu_load, traffic, latency)
                      VALUES (?, ?, ?)
                  ''', (load, traffic, latency))
                  self.conn.commit()
                  return load, traffic, latency

          class AIBrain:
              """í•™ìŠµ, ì˜ˆì¸¡, ì •ì˜¤ íŒë‹¨"""
              def __init__(self, conn):
                  self.conn = conn
                  
              def analyze_and_predict(self):
                  df = pd.read_sql_query("SELECT * FROM historical_data ORDER BY id DESC LIMIT 1000", self.conn)
                  if len(df) < 10: return None, None
                  
                  # 1. ì´ìƒ íƒì§€ (Anomaly Detection)
                  model_iso = IsolationForest(contamination=0.05)
                  df['anomaly'] = model_iso.fit_predict(df[['cpu_load', 'latency']])
                  latest_anomaly = df.iloc[-1]['anomaly'] # -1: ì´ìƒ, 1: ì •ìƒ
                  
                  # 2. ë¯¸ë˜ ì˜ˆì¸¡ (Linear Regression)
                  # ê°„ë‹¨í•œ ì‹œê³„ì—´ ì˜ˆì¸¡ (ë‹¤ìŒ Stepì˜ ë¶€í•˜ ì˜ˆì¸¡)
                  X = np.array(range(len(df))).reshape(-1, 1)
                  y = df['cpu_load'].values
                  model_lr = LinearRegression()
                  model_lr.fit(X, y)
                  prediction = model_lr.predict([[len(df) + 1]])[0]
                  
                  return latest_anomaly, prediction

          class NewsEditor:
              """ì£¼ê°„ ë‰´ìŠ¤ ë° ë¦¬í¬íŠ¸ ìƒì„±"""
              def __init__(self, conn):
                  self.conn = conn
                  
              def publish_daily_report(self, load, anomaly, pred):
                  status = "âœ… STABLE" if anomaly == 1 else "ğŸš¨ ABNORMAL DETECTED"
                  with open(DAILY_REPORT, 'w') as f:
                      f.write(f"# ğŸ“Š Daily AI Insight\n")
                      f.write(f"**Generated:** {datetime.datetime.now()}\n\n")
                      f.write(f"## ğŸ“¡ Real-time Status\n")
                      f.write(f"- **Current Load:** {load:.2f}%\n")
                      f.write(f"- **AI Judgment:** {status}\n")
                      f.write(f"- **Next Step Prediction:** {pred:.2f}% Load\n")
              
              def publish_weekly_news(self):
                  # ë§¤ì£¼ ì›”ìš”ì¼(weekday=0)ì—ë§Œ ë‰´ìŠ¤ ë°œí–‰
                  if datetime.datetime.now().weekday() != 0:
                      return
                      
                  # ì§€ë‚œ 7ì¼ê°„ì˜ ë°ì´í„° ì§‘ê³„
                  df = pd.read_sql_query("SELECT * FROM historical_data WHERE timestamp >= date('now', '-7 days')", self.conn)
                  if df.empty: return

                  avg_load = df['cpu_load'].mean()
                  max_traffic = df['traffic'].max()
                  total_rows = len(df)
                  
                  with open(WEEKLY_NEWS, 'w') as f:
                      f.write(f"# ğŸ“° Weekly AI Intelligence News\n")
                      f.write(f"**Issue Date:** {datetime.datetime.now().strftime('%Y-%m-%d')}\n\n")
                      f.write(f"## ğŸ“ˆ Weekly Summary\n")
                      f.write(f"This week, the AI server collected **{total_rows}** data points.\n\n")
                      f.write(f"| Metric | Average | Max | Min |\n")
                      f.write(f"|---|---|---|---|\n")
                      f.write(f"| CPU Load | {avg_load:.2f}% | {df['cpu_load'].max():.2f}% | {df['cpu_load'].min():.2f}% |\n")
                      f.write(f"| Traffic | {df['traffic'].mean():.0f} hits | {max_traffic} hits | {df['traffic'].min()} hits |\n")
                      f.write(f"\n## ğŸ§  AI Learning Update\n")
                      f.write(f"- The AI model has updated its baseline parameters based on {total_rows} new records.\n")
                      f.write(f"- Audit System verified integrity of all logs.\n")
                      
                  print("ğŸ“° Weekly News Published!")

          def init_db(conn):
              cursor = conn.cursor()
              # ë°ì´í„° ëˆ„ì  í…Œì´ë¸”
              cursor.execute('''
                  CREATE TABLE IF NOT EXISTS historical_data (
                      id INTEGER PRIMARY KEY AUTOINCREMENT,
                      cpu_load REAL,
                      traffic INTEGER,
                      latency REAL,
                      timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                  )
              ''')
              # ê°ì‚¬ ë¡œê·¸ í…Œì´ë¸”
              cursor.execute('''
                  CREATE TABLE IF NOT EXISTS audit_trail (
                      id INTEGER PRIMARY KEY AUTOINCREMENT,
                      actor TEXT,
                      action TEXT,
                      details TEXT,
                      data_hash TEXT,
                      timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                  )
              ''')
              conn.commit()

          def main():
              print("ğŸš€ AI Server Core Initiated.")
              if not os.path.exists("data"): os.makedirs("data")
              
              conn = sqlite3.connect(DB_PATH)
              init_db(conn)
              
              # ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤í™”
              collector = DataCollector(conn)
              brain = AIBrain(conn)
              audit = AuditLogger(conn)
              editor = NewsEditor(conn)
              
              # 1. ë°ì´í„° ìˆ˜ì§‘ ë° ëˆ„ì 
              load, _, _ = collector.collect_and_accumulate()
              audit.log_action("AI_Agent", "COLLECT", f"Load: {load:.2f}")
              
              # 2. AI ë¶„ì„ ë° ì˜ˆì¸¡
              anomaly, pred = brain.analyze_and_predict()
              if pred:
                  audit.log_action("AI_Brain", "PREDICT", f"Predicted: {pred:.2f}")
              
              # 3. ë¦¬í¬íŠ¸ ë° ë‰´ìŠ¤ ë°œí–‰
              editor.publish_daily_report(load, anomaly, pred)
              editor.publish_weekly_news() # ì¡°ê±´ ë§ì„ ë•Œë§Œ ìƒì„±
              
              conn.close()
              print("âœ… Server Tasks Completed.")

          if __name__ == "__main__":
              main()
          EOF

      # 3. AI ì„œë²„ ì—”ì§„ ì‹¤í–‰
      - name: ğŸš€ Run AI Hyper-Server
        run: |
          python scripts/ai_server_core.py
          
          echo "ğŸ“„ Preview Daily Insight:"
          cat data/daily_insight.md
          
          if [ -f "data/weekly_ai_news.md" ]; then
            echo "ğŸ“° WEEKLY NEWS DETECTED:"
            cat data/weekly_ai_news.md
          fi

      # 4. ë°ì´í„° ë³´ì•ˆ ë°±ì—… (ì•”í˜¸í™”)
      - name: ğŸ”’ Encrypt & Backup Assets
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          cp "$DB_PATH" "$BACKUP_DIR/server_$TIMESTAMP.bak"
          
          echo "ğŸ” Encrypting Server Data..."
          zip -e -P "$ARTIFACT_PASSWORD" ai_server_storage.zip data/* backup/*
          
          # ë°±ì—… ìœ ì§€ ê´€ë¦¬ (ìµœê·¼ 5ê°œ)
          cd "$BACKUP_DIR" && ls -t *.bak | tail -n +6 | xargs -r rm --

      - name: Upload Secured Server Data
        uses: actions/upload-artifact@v4
        with:
          name: ai-server-full-data
          path: ai_server_storage.zip
          retention-days: 1

      # 5. [ì¤‘ìš”] ëˆ„ì  ë°ì´í„° ë° ë‰´ìŠ¤ ë™ê¸°í™”
      - name: ğŸ”„ Sync Server State
        run: |
          git config --global user.name "AI-Server-Bot"
          git config --global user.email "server@grand-ops.ai"
          git config --global --add safe.directory $GITHUB_WORKSPACE

          echo "ğŸ“¦ Staging Accumulated Data..."
          
          # íŒŒì¼ ì¡´ì¬ í™•ì¸ í›„ ê°•ì œ ì¶”ê°€ (ì•ˆì „ì„± ë³´ì¥)
          [ -f "data/grand_ops_server.db" ] && git add -f data/grand_ops_server.db
          [ -f "data/daily_insight.md" ] && git add -f data/daily_insight.md
          [ -f "data/weekly_ai_news.md" ] && git add -f data/weekly_ai_news.md
          
          git add -A

          if [ -z "$(git status --porcelain)" ]; then
             echo "âœ… Server state is current."
             exit 0
          fi

          echo "ğŸ’¾ Committing Intelligence..."
          git commit -m "server: data accum & news update [skip ci]"

          # ë¬´í•œ ì¬ì‹œë„ ë£¨í”„ (ë™ê¸°í™” ë³´ì¥)
          MAX_RETRIES=5
          COUNT=0
          while [ $COUNT -lt $MAX_RETRIES ]; do
            git pull --rebase origin main || git rebase --abort
            if git push origin main; then
              echo "ğŸš€ Server Synced Successfully!"
              exit 0
            fi
            sleep 5
            COUNT=$((COUNT+1))
          done
          exit 1
